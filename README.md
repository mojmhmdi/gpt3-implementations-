you can find the GPT3 implementation (The encoder-decoder transformer model) in a single file "GPT3.py"

there are two examples datasets: one is shekspear and the other is Farsi poem. 

an example of the model outputs trained on Farsi poem is posted here :
https://www.linkedin.com/feed/update/urn:li:activity:7032459371112493056/

Dsiclaimer: some parts of this code have been inspied from amazing githb page of Andrej karpathy.
